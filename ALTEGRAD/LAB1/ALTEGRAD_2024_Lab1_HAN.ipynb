{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuVouapRmjEW"
      },
      "source": [
        "<center><h2>ALTeGraD 2024<br>Lab Session 1: HAN</h2><h3>Hierarchical Attention Network Using GRU</h3> 8 / 10 / 2024<br> Dr. Guokan Shang, Yang Zhang<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> [fill me]\n",
        "\n",
        "\n",
        "</center>\n",
        "In this lab, you will get familiar with recurrent neural networks (RNNs), self-attention, and the HAN architecture <b>(Yang et al. 2016)</b> using PyTorch. In this architecture, sentence embeddings are first individually produced, and a document embedding is then computed from the sentence embeddings.<br>\n",
        "<b>The deadline for this lab is October 15, 2024 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJaSJaIP1xRy"
      },
      "source": [
        "### = = = = =  Attention Layer = = = = =\n",
        "In thi section, you will fill the gaps in the code to implement the self-attention layer. This layer will be used later to define the HAN architecture. The basic idea behind attention is that rather than considering the last annotation $h_T$ as a summary of the entire sequence, which is prone to information loss, the annotations at <i>all</i> time steps are used.\n",
        "The self-attention mechanism computes a weighted sum of the annotations, where the weights are determined by trainable parameters. Refer to <b>section 2.2</b> in the handout for the theoretical part, it will be needed to finish the first task.\n",
        "\n",
        "#### <b>Task 1:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "import json\n",
        "import operator\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using mps device\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps (I have a MAC) device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yoM7H0KQncpF"
      },
      "outputs": [],
      "source": [
        "class AttentionWithContext(nn.Module):\n",
        "    \"\"\"\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    \n",
        "    # Input shape\n",
        "    Args:\n",
        "        3D tensor with shape: `(samples, steps, features)`\n",
        "    # Output shape\n",
        "    Return:\n",
        "        2D tensor with shape: `(samples, features)`\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_shape, return_coefficients=False, bias=True):\n",
        "        super(AttentionWithContext, self).__init__()\n",
        "        self.return_coefficients = return_coefficients\n",
        "\n",
        "        self.W = nn.Linear(input_shape, input_shape, bias=bias)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.u = nn.Linear(input_shape, 1, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.W.weight.data.uniform_(-initrange, initrange)\n",
        "        self.W.bias.data.uniform_(-initrange, initrange)\n",
        "        self.u.weight.data.uniform_(-initrange, initrange)\n",
        "    \n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        # do not pass the mask to the next layers\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        uit = self.W(x) # fill the gap # compute uit = W . x  where x represents ht\n",
        "        uit = self.tanh(uit)\n",
        "        ait = self.u(uit)\n",
        "        a = torch.exp(ait)\n",
        "        \n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            a = a*mask.double()\n",
        "        \n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
        "        eps = 1e-9\n",
        "        a = a / (torch.sum(a, axis=1, keepdim=True) + eps)\n",
        "        weighted_input = torch.sum(a * x, axis=1, keepdim=True)    ### fill the gap ### # compute the attentional vector\n",
        "        if self.return_coefficients:\n",
        "            return  weighted_input, a    ### [attentional vector, coefficients] ### use torch.sum to compute s\n",
        "        else:\n",
        "            return weighted_input    ### attentional vector only ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgTP6GrOHlss"
      },
      "source": [
        "### = = = = = Parameters = = = = =\n",
        "In this section, we define the parameters to use in our training. Such as data path, the embedding dimention <b>d</b>, the GRU layer dimensionality <b>n_units</b>, etc..<br>\n",
        "The parameter <b>device</b> is used to train the model on GPU if it is available. for this purpose, if you are using Google Colab, switch your runtime to a GPU runtime to train the model with a maximum speed.<br>\n",
        "<b>Bonus question:</b> What is the purpose of the parameter <i>my_patience</i>?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "czsVjxgYnczb"
      },
      "outputs": [],
      "source": [
        "path_root = ''\n",
        "path_to_data = path_root + 'data/'\n",
        "\n",
        "d = 30 # dimensionality of word embeddings\n",
        "n_units = 50 # RNN layer dimensionality\n",
        "drop_rate = 0.5 # dropout\n",
        "mfw_idx = 2 # index of the most frequent words in the dictionary \n",
        "            # 0 is for the special padding token\n",
        "            # 1 is for the special out-of-vocabulary token\n",
        "\n",
        "padding_idx = 0\n",
        "oov_idx = 1\n",
        "batch_size = 64\n",
        "nb_epochs = 15\n",
        "my_patience = 2 # for early stopping strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Vot_C7Hlst"
      },
      "source": [
        "### = = = = = Data Loading = = = = =\n",
        "In this section we will use first <b>wget</b> to download the data the we will load it using numpy in the first cell. While in the second cell, we will use these data to define our Pytorch data loader. Note that the data is already preprocessed, tokenized and padded.<br><br>\n",
        "<b>Note: if you are running your notebook on Windows or on MacOS, <i>wget</i> will probably not work if you did not install it manually. In this case, use the provided link to download the data and change the <i>path_to_data</i> in the <i>Parameters</i> section accordingly. Otherwise, you will face no problem on Ubuntu and Google Colab.</b>\n",
        "\n",
        "#### <b>Task 2.1:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UD6hRh0OHlst"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "replace __MACOSX/._data? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
          ]
        }
      ],
      "source": [
        "url = \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199289&authkey=AHgxt3xmgG0Fu5A\"\n",
        "output_file = \"data.zip\"\n",
        "urllib.request.urlretrieve(url, output_file)\n",
        "\n",
        "!unzip data.zip\n",
        "\n",
        "my_docs_array_train = np.load(path_to_data + 'docs_train.npy')\n",
        "my_docs_array_test = np.load(path_to_data + 'docs_test.npy')\n",
        "\n",
        "my_labels_array_train = np.load(path_to_data + 'labels_train.npy')\n",
        "my_labels_array_test = np.load(path_to_data + 'labels_test.npy')\n",
        "\n",
        "# load dictionary of word indexes (sorted by decreasing frequency across the corpus)\n",
        "with open(path_to_data + 'word_to_index.json', 'r') as my_file:\n",
        "    word_to_index = json.load(my_file)\n",
        "\n",
        "# invert mapping\n",
        "index_to_word =   {index : word for word, index in word_to_index.items()}    ### fill the gap (use a dict comprehension) ###\n",
        "input_size = my_docs_array_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DpsCvmaiJfZc"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class Dataset_(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.documents = x\n",
        "        self.labels = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        document = self.documents[index]\n",
        "        label = self.labels[index] \n",
        "        sample = {\n",
        "            \"document\": torch.tensor(document),\n",
        "            \"label\": torch.tensor(label),\n",
        "            }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def get_loader(x, y, batch_size=32):\n",
        "    dataset = Dataset_(x, y)\n",
        "    data_loader = DataLoader(dataset=dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True,\n",
        "                            pin_memory=True,\n",
        "                            drop_last=True,\n",
        "                            )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rzqEGOdHlst"
      },
      "source": [
        "### = = = = = Defining Architecture = = = = =\n",
        "In this section, we define the HAN architecture. We start with <i>AttentionBiGRU</i> module in order to define the sentence encoder (check Figure 3 in the handout). Then, we define the <i>TimeDistributed</i> module to allow us to forward our input (batch of document) as to the sentence encoder as <b>batch of sentences</b>, where each sentence in the document will be considered as a time step. This module also reshape the output to a batch of timesteps representations per document. Finally we define the <b>HAN</b> architecture using <i>TimeDistributed</i>, <i>AttentionWithContext</i> and <i>GRU</i>.\n",
        "\n",
        "#### <b>Task 2.2:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AMj9j1_pHlst"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AttentionBiGRU(nn.Module):\n",
        "    def __init__(self, input_shape, n_units, index_to_word, dropout=0):\n",
        "        super(AttentionBiGRU, self).__init__()    #RNN bidirectionnel (GRU)\n",
        "        self.embedding = nn.Embedding(len(index_to_word),    # fill the gap # vocab size\n",
        "                                      d, # dimensionality of embedding space\n",
        "                                      padding_idx=0)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.gru = nn.GRU(input_size=d,\n",
        "                          hidden_size=n_units,\n",
        "                          num_layers=1,\n",
        "                          bias=True,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        self.attention = AttentionWithContext(n_units * 2,   # fill the gap # the input shape for the attention layer # 2 * n_units because of bidirectional GRU\n",
        "                                              return_coefficients=True)\n",
        "\n",
        "\n",
        "    def forward(self, sent_ints):\n",
        "        sent_wv = self.embedding(sent_ints)\n",
        "        sent_wv_dr = self.dropout(sent_wv)\n",
        "        sent_wa, _ = self.gru(sent_wv_dr) # fill the gap # RNN layer\n",
        "        sent_att_vec, word_att_coeffs =  self.attention(sent_wa)    # fill the gap # attentional vector for the sent\n",
        "        sent_att_vec_dr = self.dropout(sent_att_vec)     \n",
        "        return sent_att_vec_dr, word_att_coeffs\n",
        "\n",
        "class TimeDistributed(nn.Module):\n",
        "    \"\"\" \n",
        "    Apply AttentionBiGRU on each sentence in the document \"\"\"\n",
        "    def __init__(self, module, batch_first=False):\n",
        "        super(TimeDistributed, self).__init__()\n",
        "        self.module = module\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.size()) <= 2:\n",
        "            return self.module(x)\n",
        "        # Squash samples and timesteps into a single axis\n",
        "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size) (448, 30)\n",
        "        sent_att_vec_dr, word_att_coeffs = self.module(x_reshape)\n",
        "        # We have to reshape the output\n",
        "        if self.batch_first:\n",
        "            sent_att_vec_dr = sent_att_vec_dr.contiguous().view(x.size(0), -1, sent_att_vec_dr.size(-1))  # (samples, timesteps, output_size)\n",
        "            word_att_coeffs = word_att_coeffs.contiguous().view(x.size(0), -1, word_att_coeffs.size(-1))  # (samples, timesteps, output_size)\n",
        "        else:\n",
        "            sent_att_vec_dr = sent_att_vec_dr.view(-1, x.size(1), sent_att_vec_dr.size(-1))  # (timesteps, samples, output_size)\n",
        "            word_att_coeffs = word_att_coeffs.view(-1, x.size(1), word_att_coeffs.size(-1))  # (timesteps, samples, output_size)\n",
        "        return sent_att_vec_dr, word_att_coeffs      \n",
        "\n",
        "class HAN(nn.Module):\n",
        "    def __init__(self, input_shape, n_units, index_to_word, dropout=0):\n",
        "        super(HAN, self).__init__()\n",
        "        self.encoder = AttentionBiGRU(input_shape, n_units, index_to_word, dropout)\n",
        "        self.timeDistributed = TimeDistributed(self.encoder, True)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "        self.gru = nn.GRU(input_size = n_units * 2,    # fill the gap # the input shape of GRU layer\n",
        "                          hidden_size=n_units,\n",
        "                          num_layers=1,\n",
        "                          bias=True,\n",
        "                          batch_first=True,\n",
        "                          bidirectional=True)\n",
        "        self.attention = AttentionWithContext(n_units * 2,     # fill the gap # the input shape of between-sentence attention layer\n",
        "                                              return_coefficients=True)\n",
        "        self.lin_out = nn.Linear(n_units * 2,   # fill the gap # the input size of the last linear layer\n",
        "                                 1)\n",
        "        self.preds = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, doc_ints):\n",
        "        sent_att_vecs_dr, word_att_coeffs =  self.timeDistributed(doc_ints)    # fill the gap # get sentence representation\n",
        "        doc_sa, _ = self.gru(sent_att_vecs_dr)\n",
        "        doc_att_vec, sent_att_coeffs = self.attention(doc_sa)\n",
        "        doc_att_vec_dr = self.dropout(doc_att_vec)\n",
        "        doc_att_vec_dr = self.lin_out(doc_att_vec_dr)\n",
        "        return self.preds(doc_att_vec_dr), word_att_coeffs, sent_att_coeffs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgreR5AcHlst"
      },
      "source": [
        "### = = = = = Training = = = = =\n",
        "In this section, we have two code cells. In the first one, we define our evaluation function to compute the training and validation accuracies. While in the second one, we define our model, loss and optimizer and train the model over <i>nb_epochs</i>.<br>\n",
        "<b>Bonus task:</b> use <a href=\"https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html\" target=\"_blank\">tensorboard</a> to visualize the loss and the validation accuray during the training.\n",
        "\n",
        "#### <b>Task 2.3:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ztF2Lkie-C25"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy(data_loader, verbose=True):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    ncorrect = ntotal = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(data_loader):\n",
        "            # inference \n",
        "            output = model(data[\"document\"].to(device))[0] \n",
        "            output = output[:, -1] # only last vector\n",
        "            # total number of examples\n",
        "            ntotal +=  output.shape[0]\n",
        "            # number of correct predictions \n",
        "            predictions = torch.round(output)\n",
        "            ncorrect += torch.sum(predictions.to(device) == data[\"label\"].to(device))    #fill me # number of correct prediction - hint: use torch.sum \n",
        "        acc = ncorrect.item() / ntotal\n",
        "        if verbose:\n",
        "          print(\"validation accuracy: {:3.2f}\".format(acc*100))\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RRYiKhZEEidb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|ââââââââââ| 390/390 [00:26<00:00, 14.57batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 1 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3200.26%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|ââââââââââ| 390/390 [00:26<00:00, 14.46batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 2 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3201.03%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.38batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 3 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3199.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.36batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 4 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3198.97%\n",
            "Validation accuracy did not improve for 2 epochs, stopping training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.22batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 5 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3200.51%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|ââââââââââ| 390/390 [00:27<00:00, 13.97batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 6 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3200.26%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.08batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 7 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3201.79%\n",
            "Validation accuracy improved, saving model...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.06batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 8 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3199.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.05batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 9 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3199.23%\n",
            "Validation accuracy did not improve for 2 epochs, stopping training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.04batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 10 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3200.77%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|ââââââââââ| 390/390 [00:27<00:00, 13.99batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 11 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3199.23%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.01batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 12 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3199.23%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|ââââââââââ| 390/390 [00:28<00:00, 13.85batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 13 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3200.26%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|ââââââââââ| 390/390 [00:27<00:00, 13.98batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 14 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3198.97%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|ââââââââââ| 390/390 [00:27<00:00, 14.22batch/s, accuracy=3.2e+3, loss=0] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Epoch 15 Complete: Avg. Loss: 0.0000, Validation Accuracy: 3200.51%\n",
            "Loading best checkpoint...\n",
            "done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/t6/khg6jj9s4_b0lqrt24p4bvs00000gn/T/ipykernel_3846/2234124709.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_model.pt'))\n"
          ]
        }
      ],
      "source": [
        "model = HAN(input_size, n_units, index_to_word).to(device)\n",
        "#model = model.double()    #I have removed this line because it was causing an error\n",
        "lr = 0.001  # learning rate\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=None, #often used for multi-classes classification problems \n",
        "                                      size_average=None, \n",
        "                                      ignore_index=-100, \n",
        "                                      reduce=None, \n",
        "                                      reduction='mean', \n",
        "                                      label_smoothing=0.0)    # fill the gap, use Binary cross entropy from torch.nn: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "optimizer = torch.optim.Adam(model.parameters(), \n",
        "                             lr = lr) #fill me\n",
        "\n",
        "def train(x_train=my_docs_array_train,\n",
        "          y_train=my_labels_array_train,\n",
        "          x_test=my_docs_array_test,\n",
        "          y_test=my_labels_array_test,\n",
        "          word_dict=index_to_word,\n",
        "          batch_size=batch_size):\n",
        "  \n",
        "    train_data = get_loader(x_train, y_train, batch_size)\n",
        "    test_data = get_loader(my_docs_array_test, my_labels_array_test, batch_size)\n",
        "\n",
        "    best_validation_acc = 0.0\n",
        "    p = 0 # patience\n",
        "\n",
        "    for epoch in range(1, nb_epochs + 1): \n",
        "        losses = []\n",
        "        accuracies = []\n",
        "        with tqdm(train_data, unit=\"batch\") as tepoch:\n",
        "            for idx, data in enumerate(tepoch):\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                model.train()\n",
        "                optimizer.zero_grad()\n",
        "                input = data['document'].to(device)\n",
        "                label = data['label'].to(device)\n",
        "                #label = label.double()     #I have removed this line because it was causing an error\n",
        "                output = model.forward(input)[0]\n",
        "                output = output[:, -1]\n",
        "                loss = criterion(output, label)    # fill the gap # compute the loss\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
        "                optimizer.step()\n",
        "\n",
        "                losses.append(loss.item())\n",
        "                accuracy = torch.sum(torch.round(output) == label).item() / batch_size\n",
        "                accuracies.append(accuracy)\n",
        "                tepoch.set_postfix(loss=sum(losses)/len(losses), accuracy=100. * sum(accuracies)/len(accuracies))\n",
        "\n",
        "        # train_acc = evaluate_accuracy(train_data, False)\n",
        "        test_acc = evaluate_accuracy(test_data, False)\n",
        "        print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}, Validation Accuracy: {:3.2f}%\"\n",
        "              .format(epoch, sum(losses)/len(losses), 100.*test_acc))\n",
        "        if test_acc >= best_validation_acc:\n",
        "            best_validation_acc = test_acc\n",
        "            print(\"Validation accuracy improved, saving model...\")\n",
        "            torch.save(model.state_dict(), './best_model.pt')\n",
        "            p = 0\n",
        "            print()\n",
        "        else:\n",
        "            p += 1\n",
        "            if p==my_patience:\n",
        "                print(\"Validation accuracy did not improve for {} epochs, stopping training...\".format(my_patience))\n",
        "    print(\"Loading best checkpoint...\")    \n",
        "    model.load_state_dict(torch.load('./best_model.pt'))\n",
        "    model.eval()\n",
        "    print('done.')\n",
        "\n",
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvyr8B5QHlst"
      },
      "source": [
        "### = = = = = Extraction of Attention Coefficients = = = = =\n",
        "In this section, we will extract and display the attention coefficients on two levels: sentence level and word level. To do so, we will extract the corresponding weights from our model.\n",
        "#### <b>Task 3:</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UVr8cS4MHlst"
      },
      "outputs": [],
      "source": [
        "# select last review:\n",
        "my_review = my_docs_array_test[-1:,:,:]\n",
        " \n",
        "# convert integer review to text:\n",
        "index_to_word[1] = 'OOV'\n",
        "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]    #list of list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHDJ7JiqHlsu"
      },
      "source": [
        "###   &emsp;&emsp;  = = = = = Attention Over Sentences in the Document = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_tensor(review):\n",
        "    \"\"\" \n",
        "    Convert a review to a tensor\n",
        "    \"\"\"\n",
        "    all_words = [word for sentence in review for word in sentence]\n",
        "    unique_words = np.unique(all_words)\n",
        "    word_to_index = {element : indice for indice, element in enumerate(unique_words)}\n",
        "    indexed_sentences = [[word_to_index[word] for word in sentence] for sentence in review]\n",
        "    max_length = max(len(sentence) for sentence in indexed_sentences)\n",
        "    padded_sentences = [sentence + [0] * (max_length - len(sentence)) for sentence in indexed_sentences]\n",
        "    review_tensor = torch.tensor(padded_sentences, dtype=torch.long)\n",
        "    return review_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yooWg3kkHlsu",
        "outputId": "2421291a-c557-46a9-8f75-74c998053f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100.0 There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )\n"
          ]
        }
      ],
      "source": [
        "review_tensor = convert_to_tensor(my_review_text).to(device)\n",
        "preds, word_coeffs,sent_coeffs = model(review_tensor)# fill the gap # get sentence attention coeffs by passing the review to the model - (you need to convert the inout torch tensor)\n",
        "sent_coeffs = sent_coeffs[0,:,:]\n",
        "\n",
        "for elt in zip(sent_coeffs[:,0].tolist(),[' '.join(elt) for elt in my_review_text]):\n",
        "    print(round(elt[0]*100,2),elt[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rII-DNrKHlsu"
      },
      "source": [
        "### &emsp;&emsp; = = = = = Attention Over Words in Each Sentence = = = = ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyFjAga6Hlsu",
        "outputId": "d9640e73-a3a1-4745-b215-d55470036a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('There', 0.03328154236078262)\n",
            "(\"'s\", 0.0333699956536293)\n",
            "('a', 0.031412579119205475)\n",
            "('sign', 0.03244566544890404)\n",
            "('on', 0.03262363746762276)\n",
            "('The', 0.03413810953497887)\n",
            "('Lost', 0.0310550257563591)\n",
            "('Highway', 0.027628034353256226)\n",
            "('that', 0.032391179352998734)\n",
            "('says', 0.03208443522453308)\n",
            "(':', 0.03371325880289078)\n",
            "('OOV', 0.032864101231098175)\n",
            "('SPOILERS', 0.03439488261938095)\n",
            "('OOV', 0.032609280198812485)\n",
            "('(', 0.03145695850253105)\n",
            "('but', 0.03449395298957825)\n",
            "('you', 0.033434413373470306)\n",
            "('already', 0.03440007194876671)\n",
            "('knew', 0.03755567967891693)\n",
            "('that', 0.03462287783622742)\n",
            "(',', 0.03398417681455612)\n",
            "('did', 0.033457618206739426)\n",
            "(\"n't\", 0.031061196699738503)\n",
            "('you', 0.033078633248806)\n",
            "('?', 0.034328561276197433)\n",
            "(')', 0.03871575742959976)\n",
            "= = = =\n",
            "('Since', 0.03302547335624695)\n",
            "('there', 0.032407186925411224)\n",
            "(\"'s\", 0.0318182110786438)\n",
            "('a', 0.02996838092803955)\n",
            "('great', 0.03069513477385044)\n",
            "('deal', 0.034154802560806274)\n",
            "('of', 0.03262675553560257)\n",
            "('people', 0.032101571559906006)\n",
            "('that', 0.03292544558644295)\n",
            "('apparently', 0.03374908119440079)\n",
            "('did', 0.035708751529455185)\n",
            "('not', 0.03762490674853325)\n",
            "('get', 0.03850315883755684)\n",
            "('the', 0.03677520528435707)\n",
            "('point', 0.03442980721592903)\n",
            "('of', 0.03492303565144539)\n",
            "('this', 0.031077811494469643)\n",
            "('movie', 0.03248457610607147)\n",
            "(',', 0.03226258605718613)\n",
            "('I', 0.030487610027194023)\n",
            "(\"'d\", 0.030328067019581795)\n",
            "('like', 0.03155135735869408)\n",
            "('to', 0.030520688742399216)\n",
            "('contribute', 0.029470695182681084)\n",
            "('my', 0.034092023968696594)\n",
            "('interpretation', 0.03745589405298233)\n",
            "('of', 0.03427290543913841)\n",
            "('why', 0.0336691290140152)\n",
            "('the', 0.03579133003950119)\n",
            "('plot', 0.035098426043987274)\n",
            "= = = =\n",
            "('As', 0.03128800168633461)\n",
            "('others', 0.03061799332499504)\n",
            "('have', 0.03493322432041168)\n",
            "('pointed', 0.03722050040960312)\n",
            "('out', 0.03527166321873665)\n",
            "(',', 0.034427460283041)\n",
            "('one', 0.03797880560159683)\n",
            "('single', 0.033043958246707916)\n",
            "('viewing', 0.036142367869615555)\n",
            "('of', 0.03294754773378372)\n",
            "('this', 0.029836902394890785)\n",
            "('movie', 0.032443542033433914)\n",
            "('is', 0.03410245105624199)\n",
            "('not', 0.03696523606777191)\n",
            "('sufficient', 0.03333483636379242)\n",
            "('.', 0.03254655748605728)\n",
            "= = = =\n",
            "('If', 0.03141407296061516)\n",
            "('you', 0.0340745784342289)\n",
            "('have', 0.03551186993718147)\n",
            "('the', 0.03729679062962532)\n",
            "('DVD', 0.03433481603860855)\n",
            "('of', 0.03439571335911751)\n",
            "('MD', 0.03193951025605202)\n",
            "(',', 0.031167740002274513)\n",
            "('you', 0.033018454909324646)\n",
            "('can', 0.03390618413686752)\n",
            "('OOV', 0.03389271721243858)\n",
            "(\"'\", 0.0339304655790329)\n",
            "('by', 0.037795308977365494)\n",
            "('looking', 0.03247794881463051)\n",
            "('at', 0.03639926016330719)\n",
            "('David', 0.035719819366931915)\n",
            "('Lynch', 0.03352368623018265)\n",
            "(\"'s\", 0.033917445689439774)\n",
            "(\"'Top\", 0.030905364081263542)\n",
            "('10', 0.0320635586977005)\n",
            "('OOV', 0.032231517136096954)\n",
            "('to', 0.03132383152842522)\n",
            "('OOV', 0.03347159922122955)\n",
            "('MD', 0.031719401478767395)\n",
            "(\"'\", 0.03083132952451706)\n",
            "('(', 0.03081524185836315)\n",
            "('but', 0.03493312746286392)\n",
            "('only', 0.033602889627218246)\n",
            "('upon', 0.030338339507579803)\n",
            "('second', 0.03304743021726608)\n",
            "= = = =\n",
            "(';', 0.0330420583486557)\n",
            "(')', 0.03746476769447327)\n",
            "('First', 0.032882630825042725)\n",
            "('of', 0.03328603878617287)\n",
            "('all', 0.03345033898949623)\n",
            "(',', 0.033038429915905)\n",
            "('Mulholland', 0.03401168808341026)\n",
            "('Drive', 0.03478190675377846)\n",
            "('is', 0.03357034921646118)\n",
            "('downright', 0.03361755236983299)\n",
            "('brilliant', 0.03246937319636345)\n",
            "('.', 0.033457860350608826)\n",
            "= = = =\n",
            "('A', 0.03450243920087814)\n",
            "('masterpiece', 0.03745193034410477)\n",
            "('.', 0.03373100236058235)\n",
            "= = = =\n",
            "('This', 0.03782075643539429)\n",
            "('is', 0.03542785719037056)\n",
            "('the', 0.037267059087753296)\n",
            "('kind', 0.03500010818243027)\n",
            "('of', 0.034369636327028275)\n",
            "('movie', 0.03462187573313713)\n",
            "('that', 0.034482698887586594)\n",
            "('refuse', 0.03248430788516998)\n",
            "('to', 0.0309250857681036)\n",
            "('leave', 0.029847526922822)\n",
            "('your', 0.02823062054812908)\n",
            "('head', 0.03072412684559822)\n",
            "('.', 0.032111283391714096)\n",
            "= = = =\n",
            "(')', 0.03871575742959976)\n",
            "('knew', 0.03755567967891693)\n",
            "('that', 0.03462287783622742)\n",
            "('but', 0.03449395298957825)\n",
            "('already', 0.03440007194876671)\n",
            "('SPOILERS', 0.03439488261938095)\n",
            "('?', 0.034328561276197433)\n",
            "('The', 0.03413810953497887)\n",
            "(',', 0.03398417681455612)\n",
            "(':', 0.03371325880289078)\n",
            "('did', 0.033457618206739426)\n",
            "('you', 0.033434413373470306)\n",
            "(\"'s\", 0.0333699956536293)\n",
            "('There', 0.03328154236078262)\n",
            "('you', 0.033078633248806)\n",
            "('OOV', 0.032864101231098175)\n",
            "('on', 0.03262363746762276)\n",
            "('OOV', 0.032609280198812485)\n",
            "('sign', 0.03244566544890404)\n",
            "('that', 0.032391179352998734)\n",
            "('says', 0.03208443522453308)\n",
            "('(', 0.03145695850253105)\n",
            "('a', 0.031412579119205475)\n",
            "(\"n't\", 0.031061196699738503)\n",
            "('Lost', 0.0310550257563591)\n",
            "('Highway', 0.027628034353256226)\n",
            "= = = =\n",
            "('get', 0.03850315883755684)\n",
            "('not', 0.03762490674853325)\n",
            "('interpretation', 0.03745589405298233)\n",
            "('the', 0.03677520528435707)\n",
            "('the', 0.03579133003950119)\n",
            "('did', 0.035708751529455185)\n",
            "('plot', 0.035098426043987274)\n",
            "('of', 0.03492303565144539)\n",
            "('point', 0.03442980721592903)\n",
            "('of', 0.03427290543913841)\n",
            "('deal', 0.034154802560806274)\n",
            "('my', 0.034092023968696594)\n",
            "('apparently', 0.03374908119440079)\n",
            "('why', 0.0336691290140152)\n",
            "('Since', 0.03302547335624695)\n",
            "('that', 0.03292544558644295)\n",
            "('of', 0.03262675553560257)\n",
            "('movie', 0.03248457610607147)\n",
            "('there', 0.032407186925411224)\n",
            "(',', 0.03226258605718613)\n",
            "('people', 0.032101571559906006)\n",
            "(\"'s\", 0.0318182110786438)\n",
            "('like', 0.03155135735869408)\n",
            "('this', 0.031077811494469643)\n",
            "('great', 0.03069513477385044)\n",
            "('to', 0.030520688742399216)\n",
            "('I', 0.030487610027194023)\n",
            "(\"'d\", 0.030328067019581795)\n",
            "('a', 0.02996838092803955)\n",
            "('contribute', 0.029470695182681084)\n",
            "= = = =\n",
            "('one', 0.03797880560159683)\n",
            "('pointed', 0.03722050040960312)\n",
            "('not', 0.03696523606777191)\n",
            "('viewing', 0.036142367869615555)\n",
            "('out', 0.03527166321873665)\n",
            "('have', 0.03493322432041168)\n",
            "(',', 0.034427460283041)\n",
            "('is', 0.03410245105624199)\n",
            "('sufficient', 0.03333483636379242)\n",
            "('single', 0.033043958246707916)\n",
            "('of', 0.03294754773378372)\n",
            "('.', 0.03254655748605728)\n",
            "('movie', 0.032443542033433914)\n",
            "('As', 0.03128800168633461)\n",
            "('others', 0.03061799332499504)\n",
            "('this', 0.029836902394890785)\n",
            "= = = =\n",
            "('by', 0.037795308977365494)\n",
            "('the', 0.03729679062962532)\n",
            "('at', 0.03639926016330719)\n",
            "('David', 0.035719819366931915)\n",
            "('have', 0.03551186993718147)\n",
            "('but', 0.03493312746286392)\n",
            "('of', 0.03439571335911751)\n",
            "('DVD', 0.03433481603860855)\n",
            "('you', 0.0340745784342289)\n",
            "(\"'\", 0.0339304655790329)\n",
            "(\"'s\", 0.033917445689439774)\n",
            "('can', 0.03390618413686752)\n",
            "('OOV', 0.03389271721243858)\n",
            "('only', 0.033602889627218246)\n",
            "('Lynch', 0.03352368623018265)\n",
            "('OOV', 0.03347159922122955)\n",
            "('second', 0.03304743021726608)\n",
            "('you', 0.033018454909324646)\n",
            "('looking', 0.03247794881463051)\n",
            "('OOV', 0.032231517136096954)\n",
            "('10', 0.0320635586977005)\n",
            "('MD', 0.03193951025605202)\n",
            "('MD', 0.031719401478767395)\n",
            "('If', 0.03141407296061516)\n",
            "('to', 0.03132383152842522)\n",
            "(',', 0.031167740002274513)\n",
            "(\"'Top\", 0.030905364081263542)\n",
            "(\"'\", 0.03083132952451706)\n",
            "('(', 0.03081524185836315)\n",
            "('upon', 0.030338339507579803)\n",
            "= = = =\n",
            "(')', 0.03746476769447327)\n",
            "('Drive', 0.03478190675377846)\n",
            "('Mulholland', 0.03401168808341026)\n",
            "('downright', 0.03361755236983299)\n",
            "('is', 0.03357034921646118)\n",
            "('.', 0.033457860350608826)\n",
            "('all', 0.03345033898949623)\n",
            "('of', 0.03328603878617287)\n",
            "(';', 0.0330420583486557)\n",
            "(',', 0.033038429915905)\n",
            "('First', 0.032882630825042725)\n",
            "('brilliant', 0.03246937319636345)\n",
            "= = = =\n",
            "('masterpiece', 0.03745193034410477)\n",
            "('A', 0.03450243920087814)\n",
            "('.', 0.03373100236058235)\n",
            "= = = =\n",
            "('This', 0.03782075643539429)\n",
            "('the', 0.037267059087753296)\n",
            "('is', 0.03542785719037056)\n",
            "('kind', 0.03500010818243027)\n",
            "('movie', 0.03462187573313713)\n",
            "('that', 0.034482698887586594)\n",
            "('of', 0.034369636327028275)\n",
            "('refuse', 0.03248430788516998)\n",
            "('.', 0.032111283391714096)\n",
            "('to', 0.0309250857681036)\n",
            "('head', 0.03072412684559822)\n",
            "('leave', 0.029847526922822)\n",
            "('your', 0.02823062054812908)\n",
            "= = = =\n"
          ]
        }
      ],
      "source": [
        "#Look at the previous cell # fill the gap # get words attention coeffs by passing the review to the model - (you need to convert the inout torch tensor)\n",
        "\n",
        "word_coeffs_list = word_coeffs.reshape(7,30).tolist()\n",
        "\n",
        "# match text and coefficients:\n",
        "text_word_coeffs = [list(zip(words,word_coeffs_list[idx][:len(words)])) for idx,words in enumerate(my_review_text)]\n",
        "\n",
        "for sent in text_word_coeffs:\n",
        "    [print(elt) for elt in sent]\n",
        "    print('= = = =')\n",
        "\n",
        "# sort words by importance within each sentence:\n",
        "text_word_coeffs_sorted = [sorted(elt,key=operator.itemgetter(1),reverse=True) for elt in text_word_coeffs]\n",
        "\n",
        "for sent in text_word_coeffs_sorted:\n",
        "    [print(elt) for elt in sent]\n",
        "    print('= = = =')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1:\n",
            "): 3.87%\n",
            "knew: 3.76%\n",
            "that: 3.46%\n",
            "but: 3.45%\n",
            "already: 3.44%\n",
            "SPOILERS: 3.44%\n",
            "?: 3.43%\n",
            "The: 3.41%\n",
            ",: 3.4%\n",
            ":: 3.37%\n",
            "did: 3.35%\n",
            "you: 3.34%\n",
            "'s: 3.34%\n",
            "There: 3.33%\n",
            "you: 3.31%\n",
            "OOV: 3.29%\n",
            "on: 3.26%\n",
            "OOV: 3.26%\n",
            "sign: 3.24%\n",
            "that: 3.24%\n",
            "says: 3.21%\n",
            "(: 3.15%\n",
            "a: 3.14%\n",
            "n't: 3.11%\n",
            "Lost: 3.11%\n",
            "Highway: 2.76%\n",
            "======================\n",
            "Sentence 2:\n",
            "get: 3.85%\n",
            "not: 3.76%\n",
            "interpretation: 3.75%\n",
            "the: 3.68%\n",
            "the: 3.58%\n",
            "did: 3.57%\n",
            "plot: 3.51%\n",
            "of: 3.49%\n",
            "point: 3.44%\n",
            "of: 3.43%\n",
            "deal: 3.42%\n",
            "my: 3.41%\n",
            "apparently: 3.37%\n",
            "why: 3.37%\n",
            "Since: 3.3%\n",
            "that: 3.29%\n",
            "of: 3.26%\n",
            "movie: 3.25%\n",
            "there: 3.24%\n",
            ",: 3.23%\n",
            "people: 3.21%\n",
            "'s: 3.18%\n",
            "like: 3.16%\n",
            "this: 3.11%\n",
            "great: 3.07%\n",
            "to: 3.05%\n",
            "I: 3.05%\n",
            "'d: 3.03%\n",
            "a: 3.0%\n",
            "contribute: 2.95%\n",
            "======================\n",
            "Sentence 3:\n",
            "one: 3.8%\n",
            "pointed: 3.72%\n",
            "not: 3.7%\n",
            "viewing: 3.61%\n",
            "out: 3.53%\n",
            "have: 3.49%\n",
            ",: 3.44%\n",
            "is: 3.41%\n",
            "sufficient: 3.33%\n",
            "single: 3.3%\n",
            "of: 3.29%\n",
            ".: 3.25%\n",
            "movie: 3.24%\n",
            "As: 3.13%\n",
            "others: 3.06%\n",
            "this: 2.98%\n",
            "======================\n",
            "Sentence 4:\n",
            "by: 3.78%\n",
            "the: 3.73%\n",
            "at: 3.64%\n",
            "David: 3.57%\n",
            "have: 3.55%\n",
            "but: 3.49%\n",
            "of: 3.44%\n",
            "DVD: 3.43%\n",
            "you: 3.41%\n",
            "': 3.39%\n",
            "'s: 3.39%\n",
            "can: 3.39%\n",
            "OOV: 3.39%\n",
            "only: 3.36%\n",
            "Lynch: 3.35%\n",
            "OOV: 3.35%\n",
            "second: 3.3%\n",
            "you: 3.3%\n",
            "looking: 3.25%\n",
            "OOV: 3.22%\n",
            "10: 3.21%\n",
            "MD: 3.19%\n",
            "MD: 3.17%\n",
            "If: 3.14%\n",
            "to: 3.13%\n",
            ",: 3.12%\n",
            "'Top: 3.09%\n",
            "': 3.08%\n",
            "(: 3.08%\n",
            "upon: 3.03%\n",
            "======================\n",
            "Sentence 5:\n",
            "): 3.75%\n",
            "Drive: 3.48%\n",
            "Mulholland: 3.4%\n",
            "downright: 3.36%\n",
            "is: 3.36%\n",
            ".: 3.35%\n",
            "all: 3.35%\n",
            "of: 3.33%\n",
            ";: 3.3%\n",
            ",: 3.3%\n",
            "First: 3.29%\n",
            "brilliant: 3.25%\n",
            "======================\n",
            "Sentence 6:\n",
            "masterpiece: 3.75%\n",
            "A: 3.45%\n",
            ".: 3.37%\n",
            "======================\n",
            "Sentence 7:\n",
            "This: 3.78%\n",
            "the: 3.73%\n",
            "is: 3.54%\n",
            "kind: 3.5%\n",
            "movie: 3.46%\n",
            "that: 3.45%\n",
            "of: 3.44%\n",
            "refuse: 3.25%\n",
            ".: 3.21%\n",
            "to: 3.09%\n",
            "head: 3.07%\n",
            "leave: 2.98%\n",
            "your: 2.82%\n",
            "======================\n"
          ]
        }
      ],
      "source": [
        "# Print attention coefficients for each sentence\n",
        "for idx, sent in enumerate(text_word_coeffs_sorted):\n",
        "    print(f\"Sentence {idx + 1}:\")\n",
        "    for word, coeff in sent:\n",
        "        print(f\"{word}: {round(coeff * 100, 2)}%\")\n",
        "    print('======================')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example of 1 sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This: 3.78%\n",
            "the: 3.73%\n",
            "is: 3.54%\n",
            "kind: 3.5%\n",
            "movie: 3.46%\n",
            "that: 3.45%\n",
            "of: 3.44%\n",
            "refuse: 3.25%\n",
            ".: 3.21%\n",
            "to: 3.09%\n",
            "head: 3.07%\n",
            "leave: 2.98%\n",
            "your: 2.82%\n"
          ]
        }
      ],
      "source": [
        "for word, coeff in text_word_coeffs_sorted[6]:\n",
        "        print(f\"{word}: {round(coeff * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1400x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sentence_index = 6 \n",
        "words, coeffs = zip(*text_word_coeffs_sorted[sentence_index])    # Extract words and their coefficients for the chosen sentence\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(words, coeffs, \n",
        "         color='orange')\n",
        "plt.ylabel('Attention Coefficients')\n",
        "plt.title(f'Attention Coefficients for Sentence {sentence_index + 1} (indice {sentence_index})')\n",
        "plt.grid(axis='x')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

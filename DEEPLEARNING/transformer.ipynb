{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer\n",
    "\n",
    "    Convert each word to input sequence and generate embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math \n",
    "import copy \n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dmodel = 512) -> None:\n",
    "        super(Embedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dmodel = dmodel\n",
    "        self.embed_layer = nn.Embedding(self.vocab_size, self.dmodel)\n",
    "    def forward(self, x):\n",
    "        embed_out = self.embed_layer(x)\n",
    "        return embed_out * math.sqrt(self.dmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "    \n",
    "Étant donné que notre modèle ne contient ni récurrence ni convolution, afin que le modèle puisse utiliser l'ordre de la séquence, nous devons injecter des informations sur la position relative ou absolue des tokens dans la séquence. Pour ce faire, nous ajoutons des encodages positionnels aux embeddings d'entrée au bas des piles de l'encodeur et du décodeur. Les encodages positionnels ont la même dimension $d_{\\text{modèle}}$ que les embeddings, afin que les deux puissent être additionnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model = 512) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        frequency = torch.pow(10000, -torch.arange(0, d_model, 2, dtype = torch.float) / d_model)\n",
    "        pe = torch.zeros((max_seq_len, d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * frequency)\n",
    "        pe[:, 1::2] = torch.cos(pos * frequency)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, embed_vect):\n",
    "        return embed_vect + self.pe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer\n",
    "\n",
    "- First we create three vector(Query, Key, Value) from encoder input vector. Here new vector dimension is smaller than embedding vector.\n",
    "\n",
    "- we compute the dot products of the query with all keys, divide each by square root of d_k. Here d_k is the dimension of key vector. In our case d_k = d_v = d_model/h = 64 (d_model=512).\n",
    "\n",
    "- we apply softmax function and multiply with value matrix.\n",
    "\n",
    "- Then we resize output dimension and pass through a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dmodel = 512, n_head = 8, dropout_rate = 0.2) -> None: \n",
    "        super().__init__()\n",
    "        self.dmodel = dmodel\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.head_dim = int(dmodel / n_head)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        self.w_key = nn.Linear(dmodel, dmodel)\n",
    "        self.w_query = nn.Linear(dmodel, dmodel)\n",
    "        self.w_value = nn.Linear(dmodel, dmodel)\n",
    "        self.w_output_project = nn.Linear(dmodel, dmodel)\n",
    "\n",
    "    def attention(self, key, query, value, mask = None):\n",
    "        attention_score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matmul(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: matmul(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

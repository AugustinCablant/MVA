{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding layer\n",
    "\n",
    "    Convert each word to input sequence and generate embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import math \n",
    "import copy \n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, dmodel = 512) -> None:\n",
    "        super(Embedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dmodel = dmodel    #dimension\n",
    "        self.embed_layer = nn.Embedding(self.vocab_size, self.dmodel)\n",
    "    def forward(self, x):\n",
    "        embed_out = self.embed_layer(x)\n",
    "        return embed_out * math.sqrt(self.dmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clone(module, num_clones):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(num_clones)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "    \n",
    "Étant donné que notre modèle ne contient ni récurrence ni convolution, afin que le modèle puisse utiliser l'ordre de la séquence, nous devons injecter des informations sur la position relative ou absolue des tokens dans la séquence. Pour ce faire, nous ajoutons des encodages positionnels aux embeddings d'entrée au bas des piles de l'encodeur et du décodeur. Les encodages positionnels ont la même dimension $d_{\\text{modèle}}$ que les embeddings, afin que les deux puissent être additionnés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model = 512) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        frequency = torch.pow(10000, -torch.arange(0, d_model, 2, dtype = torch.float) / d_model)\n",
    "        pe = torch.zeros((max_seq_len, d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * frequency)\n",
    "        pe[:, 1::2] = torch.cos(pos * frequency)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, embed_vect):\n",
    "        return embed_vect + self.pe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer\n",
    "\n",
    "- First we create three vector(Query, Key, Value) from encoder input vector. Here new vector dimension is smaller than embedding vector.\n",
    "\n",
    "- we compute the dot products of the query with all keys, divide each by square root of d_k. Here d_k is the dimension of key vector. In our case d_k = d_v = d_model/h = 64 (d_model=512).\n",
    "\n",
    "- we apply softmax function and multiply with value matrix.\n",
    "\n",
    "- Then we resize output dimension and pass through a linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, n_head = 8, dropout_rate = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "        self.head_dim = int(d_model / n_head)\n",
    "        self.softmax_layer = nn.Softmax(dim = -1)\n",
    "        self.w_key = nn.Linear(d_model, d_model)\n",
    "        self.w_query = nn.Linear(d_model, d_model)\n",
    "        self.w_value = nn.Linear(d_model, d_model)\n",
    "        self.output_project = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, key, query, value, mask=None):\n",
    "        # calculate attenction score\n",
    "        \n",
    "        # attention score size for encoder attention = (BS,NH,S,S) , decoder attention = (BS,NH,T,T), encoder-decoder attention = (BS,NH,T,S)\n",
    "        attention_score = torch.matmul(query, key.transpose(-2,-1)) / math.sqrt(self.head_dim)    # query = (BS,NH,S/T,HD) , key.transpose(-2,-1) = (BS,NH,HD,S/T)\n",
    "\n",
    "        # apply masking\n",
    "        if mask is not None:\n",
    "            attention_score.masked_fill(mask==torch.tensor(False),float(\"-inf\"))\n",
    "\n",
    "        # pass through softmax layer\n",
    "        attention_weight = self.softmax_layer(attention_score)\n",
    "\n",
    "        # multiply with value\n",
    "        # Final shape of score = (BS,NH,S/T,HD)\n",
    "        score = torch.matmul(attention_weight,value)\n",
    "        return score\n",
    "\n",
    "    def forward(self, key, query, value, mask=None):\n",
    "        batch_size = key.size()[0]\n",
    "\n",
    "        # dot product with weight matrices\n",
    "        # size of key/query/value = (BS,S/T,ED) ,\n",
    "        # where BS = batch size,\n",
    "        # S = Source Sequence length, T = target sequence lenth,\n",
    "        # ED = Embedding dimension,\n",
    "        # NH = Number Of Head, HD = head dimension\n",
    "        key, query, value = self.w_key(key), self.w_query(query), self.w_value(value)\n",
    "\n",
    "        # split vector by number of head and transpose\n",
    "        # size of key/query/value = (BS,NH,S/T,HD) , where BS = batch size, NH = Number Of Head, ED = Head dimension\n",
    "        key = key.view(batch_size,-1,self.n_head,self.head_dim).transpose(1, 2)\n",
    "        query = query.view(batch_size,-1,self.n_head,self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size,-1,self.n_head,self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # size of attention_score = (BS,NH,S/T,HD)\n",
    "        attention_score = self.attention(key,query,value,mask) # size - torch.Size([2, 4, 8, 64]) -> [batch_size, max_seq_len,n_head, head_dim]\n",
    "        attention_score = self.dropout(attention_score)\n",
    "\n",
    "        # concatenated output\n",
    "        attention_score = attention_score.transpose(1,2) # size = (BS,S/T,NH,HD)\n",
    "        attention_score = attention_score.reshape(batch_size,-1,self.head_dim*self.n_head) # size = (BS,S/T,ED)\n",
    "\n",
    "        # Pass through linear layer\n",
    "        attention_out = self.output_project(attention_score)\n",
    "        return attention_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position-wise Feed-Forward Networks\n",
    "\n",
    "$ \\text{FFN} (x) = \\text{ReLU} (x W_1 + b_1) W_2 + b_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model=512, dropout_rate = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        hidden_width = 4\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.linear1 = nn.Linear(d_model,d_model*hidden_width)\n",
    "        self.linear2 = nn.Linear(d_model*hidden_width, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        transfo_lineaire = self.linear1(x)    #y = xW + b\n",
    "        relu = self.relu(transfo_lineaire)   #relu = max(0,y)\n",
    "        drop = self.dropout(relu)    #Dropout désactive de manière aléatoire un certain pourcentage de neurones pour éviter l'overfitting\n",
    "        return self.linear2(drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub-Layer \n",
    "\n",
    "    Chaque bloc d'encodeur se compose de deux sous-couches : un mécanisme d'attention multi-têtes et un réseau feed-forward appliqué à chaque position. \n",
    "\n",
    "La normalisation par couche : Dans les modèles Transformer, LayerNorm est utilisée après chaque sous-couche (comme l'attention multi-têtes et le réseau feed-forward) pour normaliser les activations, ce qui aide à stabiliser l'entraînement et à améliorer les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubLayer(nn.Module):\n",
    "    def __init__(self,d_model = 512) -> None:\n",
    "        super(SubLayer,self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)    #applique une normalisation par couche \n",
    "    def forward(self, x, sub_layer_x):\n",
    "        return self.norm(x + sub_layer_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Layer\n",
    "\n",
    "    La couche d'encodeur possède deux sous-couches. La première est un mécanisme d'attention multi-tête, et la seconde est un réseau de neurones entièrement connecté et positionnel de type feed-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeurLayer(nn.Module):\n",
    "    def __init__(self,d_model,multi_head_arttention_layer,position_wise_feedforward_layer, dropout_rate = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.multi_head_arttention_layer = multi_head_arttention_layer\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.sublayer1 = SubLayer(d_model)\n",
    "\n",
    "        self.position_wise_feedforward_layer = position_wise_feedforward_layer\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.sublayer2 = SubLayer(d_model)\n",
    "    def forward(self,vec_representation,src_mask=None):\n",
    "        # compute self attention\n",
    "        attention_score =self.multi_head_arttention_layer(key = vec_representation,query = vec_representation,value = vec_representation,mask = src_mask)\n",
    "        attention_score = self.dropout1(attention_score)\n",
    "        # Layer Norm\n",
    "        attention_out = self.sublayer1(vec_representation,attention_score)\n",
    "\n",
    "        # pass Position Wise Feedforward Network\n",
    "        position_wise_feedforward_out = self.position_wise_feedforward_layer(attention_out)\n",
    "        position_wise_feedforward_out = self.dropout2(position_wise_feedforward_out)\n",
    "        # Layer Norm\n",
    "        encoder_out = self.sublayer2(attention_out,position_wise_feedforward_out)\n",
    "        return encoder_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder is composed of a stack of N = 6 identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,encoder_layer, num_layer = 6) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.encoder_layer_list = get_clone(self.encoder_layer,num_layer)\n",
    "\n",
    "    def forward(self,src_embedding,src_mask=None):\n",
    "        encoder_out = src_embedding\n",
    "        for encoder_layer in self.encoder_layer_list:\n",
    "            encoder_out = encoder_layer(encoder_out,src_mask)\n",
    "        return encoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decodeur Layer \n",
    "\n",
    "    La couche de décodeur possède trois sous-couches. En plus des deux sous-couches de chaque couche d'encodeur, le décodeur insère une troisième sous-couche, qui effectue une attention multi-tête sur la sortie de la pile d'encodeurs.\n",
    "    \n",
    "    La couche de décodeur contient deux sous-couches : l'une est une attention multi-tête masquée, et l'autre est une \"attention encodeur-décodeur\". Dans les couches d'\"attention encodeur-décodeur\", les requêtes proviennent de la couche de décodeur précédente, tandis que les clés et les valeurs de mémoire proviennent de la sortie de l'encodeur. Cela permet à chaque position dans le décodeur d'attendre sur toutes les positions de la séquence d'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,multi_head_arttention_layer,position_wise_feedforward_layer,dropout_rate = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.decoder_attention_layer = copy.deepcopy(multi_head_arttention_layer)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        self.sublayer1 = SubLayer(d_model)\n",
    "\n",
    "        self.encoder_decoder_attention_layer = copy.deepcopy(multi_head_arttention_layer)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        self.sublayer2 = SubLayer(d_model)\n",
    "\n",
    "        self.position_wise_feedforward_layer = position_wise_feedforward_layer\n",
    "        self.dropout3 = nn.Dropout(p=dropout_rate)\n",
    "        self.sublayer3 = SubLayer(d_model)\n",
    "\n",
    "    def forward(self,enc,dec,src_mask=None,target_mask=None):\n",
    "        decoder_attention_out = self.decoder_attention_layer(key = dec,query = dec,value = dec,mask = target_mask)\n",
    "        decoder_attention_out = self.dropout1(decoder_attention_out)\n",
    "        decoder_attention_out = self.sublayer1(dec,decoder_attention_out)\n",
    "\n",
    "        enc_dec_attention_out = self.encoder_decoder_attention_layer(key = enc,query = decoder_attention_out,value = enc,mask = src_mask)\n",
    "        enc_dec_attention_out = self.dropout2(enc_dec_attention_out)\n",
    "        enc_dec_attention_out = self.sublayer2(decoder_attention_out,enc_dec_attention_out)\n",
    "\n",
    "        ffn_out = self.position_wise_feedforward_layer(enc_dec_attention_out)\n",
    "        ffn_out = self.dropout2(ffn_out)\n",
    "        ffn_out = self.sublayer2(enc_dec_attention_out,ffn_out)\n",
    "\n",
    "        return ffn_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,decoder_layer,num_layer = 6) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder_layer = decoder_layer\n",
    "        self.decoder_layer_list = get_clone(self.decoder_layer,num_layer)\n",
    "        self.layer_norm = nn.LayerNorm(self.decoder_layer.d_model)\n",
    "\n",
    "    def forward(self,encoder_out_vec,decoder_embedding,src_mask=None,target_mask=None):\n",
    "        dec_out = decoder_embedding\n",
    "        for decoder_layer in self.decoder_layer_list:\n",
    "            dec_out = decoder_layer(enc = encoder_out_vec,dec = dec_out,src_mask = src_mask,target_mask = target_mask)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGenerator(nn.Module):\n",
    "    def __init__(self,d_model,target_vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model,target_vocab_size)\n",
    "        self.softmax_layer = nn.LogSoftmax(dim=-1)\n",
    "    def forward(self,target_vec_rep):\n",
    "        return self.softmax_layer(self.linear(target_vec_rep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_seq_len,\n",
    "                 trg_seq_len,\n",
    "                 d_model,\n",
    "                 num_head,\n",
    "                 dropout_rate = 0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.src_seq_len = src_seq_len\n",
    "        self.trg_seq_len = trg_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.src_embedding = Embedding(self.src_seq_len, self.d_model)\n",
    "        self.src_pe = PositionalEncoding(self.trg_seq_len, self.d_model)\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(self.d_model, self.num_head, dropout_rate)\n",
    "        self.position_wise_feedforward = PositionWiseFeedForward(self.d_model, dropout_rate)\n",
    "\n",
    "        self.encoder_layer = EncodeurLayer(d_model, self.multi_head_attention, self.position_wise_feedforward, dropout_rate)\n",
    "        self.decodeur_layer = DecoderLayer(d_model,self.multi_head_attention,self.position_wise_feedforward,dropout_rate)\n",
    "\n",
    "\n",
    "        self.encoder_block = EncoderBlock(self.encoder_layer, num_layer = 6)\n",
    "        self.decodeur_block = DecoderBlock(self.decodeur_layer, num_layer = 6)\n",
    "        self.decoder_out_gen = DecoderGenerator(self.d_model, self.trg_seq_len)\n",
    "\n",
    "    def forward(self, src_token_id, target_token_id, src_mask=None,target_mask=None):\n",
    "        encode_out = self.encode(src_token_id, src_mask)\n",
    "        decode_out = self.decode(encode_out, target_token_id, src_mask, target_mask)\n",
    "        return decode_out\n",
    "\n",
    "    def encode(self, src_token_id, src_mask):\n",
    "        embed = self.src_embedding(src_token_id) \n",
    "        pe_out = self.src_pe(embed)\n",
    "        encoder_out = self.encoder_block(pe_out, src_mask)\n",
    "        return encoder_out\n",
    "\n",
    "    def decode(self, enc_out, trg_token_ids, src_mask=None, target_mask=None):\n",
    "        embed = self.src_embedding(trg_token_ids)\n",
    "        pe_out = self.src_pe(embed)\n",
    "        decoder_out = self.decodeur_block(enc_out, pe_out, src_mask, target_mask)\n",
    "        decoder_out = self.decoder_out_gen(decoder_out)\n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m src_mask \u001b[38;5;241m=\u001b[39m get_src_mask(src_seq,PAD_IDX)\n\u001b[1;32m     33\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m get_src_mask(trg_seq,PAD_IDX)\n\u001b[0;32m---> 34\u001b[0m output \u001b[38;5;241m=\u001b[39m transformer(src_seq, trg_seq,src_mask,trg_mask)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[50], line 29\u001b[0m, in \u001b[0;36mTransformers.forward\u001b[0;34m(self, src_token_id, target_token_id, src_mask, target_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_token_id, target_token_id, src_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,target_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 29\u001b[0m     encode_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(src_token_id, src_mask)\n\u001b[1;32m     30\u001b[0m     decode_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(encode_out, target_token_id, src_mask, target_mask)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decode_out\n",
      "Cell \u001b[0;32mIn[50], line 35\u001b[0m, in \u001b[0;36mTransformers.encode\u001b[0;34m(self, src_token_id, src_mask)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_token_id, src_mask):\n\u001b[1;32m     34\u001b[0m     embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embedding(src_token_id) \n\u001b[0;32m---> 35\u001b[0m     pe_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_pe(embed)\n\u001b[1;32m     36\u001b[0m     encoder_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_block(pe_out, src_mask)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoder_out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, embed_vect)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_vect):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embed_vect \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (20) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def get_src_mask(src_token_ids_batch,pad_tok_id):\n",
    "    batch_size = src_token_ids_batch.size()[0]\n",
    "    src_mask = (src_token_ids_batch!=pad_tok_id).view(batch_size, 1, 1,-1) #SIZE = (BS,1,1,S)\n",
    "    return src_mask\n",
    "def get_trg_mask(trg_token_ids_batch,pad_tok_id):\n",
    "    batch_size = trg_token_ids_batch.size()[0]\n",
    "    seq_len = trg_token_ids_batch.size()[1]\n",
    "    trg_pad_mask = (trg_token_ids_batch!=pad_tok_id).view(batch_size, 1, 1,-1) #SIZE = (BS,1,1,T)\n",
    "    trg_look_forward = torch.triu(torch.ones(1,1,seq_len,seq_len)).transpose(2,3)\n",
    "    trg_mask = trg_pad_mask & trg_look_forward\n",
    "    return trg_mask\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "low_bound = 3\n",
    "high_bound = 15\n",
    "batch_size = 32\n",
    "src_seq_len = 10\n",
    "trg_seq_len = 15\n",
    "\n",
    "src_tensor_size = (batch_size, src_seq_len)  \n",
    "trh_tensor_size = (batch_size, trg_seq_len)  \n",
    "\n",
    "src_seq = torch.randint(3, 16, size=src_tensor_size, dtype=torch.int32)\n",
    "trg_seq = torch.randint(3, 16, size=trh_tensor_size, dtype=torch.int32)\n",
    "transformer = Transformers(\n",
    "    src_seq_len = 20,\n",
    "    trg_seq_len = 20,\n",
    "    d_model = 512,\n",
    "    num_head = 8,\n",
    "    dropout_rate = 0.2\n",
    ")\n",
    "src_mask = get_src_mask(src_seq,PAD_IDX)\n",
    "trg_mask = get_src_mask(trg_seq,PAD_IDX)\n",
    "output = transformer(src_seq, trg_seq,src_mask,trg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
